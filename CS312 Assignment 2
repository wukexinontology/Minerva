library(boot)
library(arm)
library(Metrics)
library(Matching)

##### QUESTION 1 ################################################################################## 
set.seed(1)

#STEP 1&2: data generation process for 1000 observations
###here we are using the years of education of the parents and family income to predict the years of education of the child.
###the system component represent the randomness introduced by imperfect approximation of population distribution using a regression model.
###the stochastic component represent the randomness that is inherent in nature with our predictions.
parents_years <- rnorm(1000,mean =5 ,sd=2)
family_income <- rnorm(1000,mean=100,sd=10)
systematic_component <- 1.2*parents_years+0.02*runif(1000)+0.01*parents_years*family_income
stochastic_component <- rnorm(1000,mean=2,sd=3)

child_years <- systematic_component+stochastic_component

###create a data frame of the generated observations
my_data<- data.frame(cbind(parents_years,family_income, child_years))
head(my_data)

#STEP 3: incorrect model

###fit a regression model(#incorrect model),using parent years of education and family income to predict
lm_incorrect <- lm(child_years~parents_years+family_income,data=my_data)
lm_incorrect

###simulate the regression model
sim_incorrect <- sim(lm_incorrect,n.sim=1)
sim_incorrect

sim_child_years_incorrect <-2.13+2.14*parents_years+0.04*family_income

###calculate the RMSE of simulated predictions against the response variable in the original dataset
my_data_after_sim2 <- cbind.data.frame(my_data,sim_child_years_incorrect)
rmse(my_data_after_sim2$child_years,my_data_after_sim2$sim_child_years_incorrect)

#STEP 4: correct model

###fit a regression model(#correct model),using parent years of education, family income, and their interaction to predict
lm_correct <- lm(child_years~parents_years*family_income,data=my_data)
lm_correct

###simulate the regression model
sim_correct <- sim(lm_correct,n.sim=1)
sim_correct

sim_child_years <-3.23+1.31*parents_years-0.009*family_income+0.008*parents_years*family_income

###calculate the RMSE of simulated predictions against the response variable in the original dataset
my_data_after_sim <- cbind.data.frame(my_data,sim_child_years)
rmse(my_data_after_sim$child_years,my_data_after_sim$sim_child_years)

#STEP 5:
### For the incorrect model, the RMSE is 4.816787; for the correct model, the RMSE is 3.164856. 
### The correct model has smaller RMSE because accounting for the interaction term improved the model accuracy in predicting the regression relationship between the variables.


##### QUESTION 2 ################################################################

#create a sample of data from question 1, using first 8 observations only
my_data_subset <- my_data[sample(1:nrow(my_data),8,replace=FALSE),]
my_data_subset

x <- my_data_subset$parents_years
y <- my_data_subset$child_years

#Plot with main and axis titles
#Change point shape (pch = 19) and remove frame.
plot(x, y, main = "Correlation of Years of Education Between Parents and Child",
     sub = 'Description: 8 randomly selected observations are shown in this scatterplot, showing correlation and fitted regression line between x and y axis variables', 
     font.sub = 1, 
     font.main = 2,
     xlab = "Years of Education: Parents", ylab = "Years of Education: Child",
     pch = 19, frame = FALSE)
#add regression line
abline(lm(y ~ x, data = my_data_subset), col = "blue")

#with outlier
outlier <- c(1,0,40)
my_data_outlier <- rbind(my_data_subset,outlier)
x <- my_data_outlier$parents_years
y <- my_data_outlier$child_years
# Plot with main and axis titles
# Change point shape (pch = 19) and remove frame.
plot(x, y, main = "Correlation of Years of Education Between Parents and Child with Outlier",
     sub = 'Description:An extreme case was added to the 8 previous observations, distorting the fitted regression line', 
     font.sub = 1, 
     font.main = 1,
     xlab = "Years of Education: Parents", ylab = "Years of Education: Child",
     pch = 19, frame = FALSE)
# Add regression line
abline(lm(y ~ x, data = my_data_outlier), col = "blue")


###### QUESTION 3 #####################################################################

install.packages("Matching")
library(Matching)
View(lalonde)

#STEP 1: Run a multiple linear regression
lm1<-lm(re78~age+educ+re74+re75+hisp+black, data=lalonde)
lm1

#STEP 2:
###report coefficients and R-squared
summary(lm1)

###codes that helped to calculate R-squared by hand
###tss=total sum of squares, loop over to calculate sum
tss <- c()
for(j in 1:ncol(lalonde)){
  tssVec <- c()
  for(i in 1:nrow(lalonde)){
    b <- sum(((lalonde$re78[i]) - mean(lalonde$re78))^2)
    tssVec <- c(tssVec, b)
  }
  tss <- c(tss, sum(tssVec))
}
sum(tss)

###rss=root sum of squares, loop over to calculate sum
rss <- c()
for(j in 1:ncol(lalonde)){
  rssVec <- c()
  for(i in 1:nrow(lalonde)){
    d <- sum((lalonde$re78[i]-predict_re78[i])^2)
    rssVec <- c(rssVec, d)
  }
  rss <- c(rss, sum(rssVec))
}
sum(rss)

###calculate rsq
rsq=1-(sum(rss)/sum(tss))
rsq

#STEP 3: 95% confidence interval for expected real earning in 1978

###use the multi-linear model
lm1<-lm(re78~age+educ+re74+re75+hisp+black, data=lalonde)

###simulate the regression model to get simulated coefficients 1000 times
sim.lm1 <- sim(lm1, 1000)

###build a storage matrix to store the 1000 (predicted values) * 14 (education years) values
storage.matrix.predicted <- matrix(NA, nrow = 1000, ncol = 14)
storage.matrix.expected <- matrix(NA, nrow = 1000, ncol = 14)

###build a function to get predicted value of each coefficient simulated
get_predicted_re78 <- function(coefs, people) {
  re78 <- coefs[1] + 
    people[1]*coefs[2] +
    people[2]*coefs[3] +
    people[3]*coefs[4] + 
    people[4]*coefs[5] +
    people[5]*coefs[6] + 
    people[6]*coefs[7]
  
  return(re78)
}

###repeat 1000 times for each predicted probability to get average, aka expected probability
for (m in 1:1000) {
###iterate for each education years group
  for (educ in c(3:16)) {
###generate 1000 predicted values with each simulated set of coefficients, for a particular people profile
    for (i in 1:1000) {
      person_educ = c(mean(lalonde$age), educ, mean(lalonde$re74), 
                      mean(lalonde$re75), mean(lalonde$hisp), mean(lalonde$black)) 
      storage.matrix.predicted[i, educ-2] <- get_predicted_re78(sim.lm1@coef[i, ], person_educ) 
    storage.matrix.expected[m, educ-2] <- mean(get_predicted_re78(sim.lm1@coef[m, ], person_educ) )
    }
  }
}  


###calculate the confidence interval for expected values
conf.intervals = apply(storage.matrix.expected, 2, quantile, probs = c(0.025, 0.975))
conf.intervals

###plot the confidence interval
plot(x = c(1:1000), y = c(1:1000), type = "n", xlim = c(3,16), ylim = c(0,10000), 
     main = "The 95% Confidence Interval of Expected Real Earnings in 1978", 
     xlab = "Years of Education", ylab = "Expected Values of Real Earnings in 1978 in USD")

for (educ in 3:16) {
  segments(
    x0 = educ,
    y0 = conf.intervals[1, educ-2],
    x1 = educ, 
    y1 = conf.intervals[2, educ-2],
    lwd = 20)
}


#STEP 4: 95% confidence interval for predicted real earning in 1978

###use the multi-linear model
lm1<-lm(re78~age+educ+re74+re75+hisp+black, data=lalonde)

###simulate the regression model to get simulated coefficients 1000 times
sim.lm1 <- sim(lm1, 1000)

###build a storage matrix to store the 1000 (predicted values) * 14 (education years) values
storage.matrix.predicted <- matrix(NA, nrow = 1000, ncol = 14)

###build a function to get predicted value of each coefficient simulated
get_predicted_re78 <- function(coefs, people) {
  re78 <- coefs[1] + 
    people[1]*coefs[2] +
    people[2]*coefs[3] +
    people[3]*coefs[4] + 
    people[4]*coefs[5] +
    people[5]*coefs[6] + 
    people[6]*coefs[7]
  
  return(re78)
}


###iterate for each education years group
  for (educ in c(3:16)) {
###generate 1000 predicted values with each simulated set of coefficients, for a particular people profile
    for (i in 1:1000) {
      person_educ = c(mean(lalonde$age), educ, mean(lalonde$re74), 
                      mean(lalonde$re75), mean(lalonde$hisp), mean(lalonde$black)) 
      storage.matrix.predicted[i, educ-2] <- get_predicted_re78(sim.lm1@coef[i, ], person_educ) 
      }
  }  


###calculate the confidence interval for predicted values
conf.intervals = apply(storage.matrix.predicted, 2, quantile, probs = c(0.025, 0.975))
conf.intervals

###plot the confidence interval
plot(x = c(1:1000), y = c(1:1000), type = "n", xlim = c(3,16), ylim = c(0,10000), 
     main = "The 95% Confidence Interval of Predicted Real Earnings in 1978", 
     xlab = "Years of Education", ylab = "Predicted Values of Real Earnings in 1978 in USD")

for (educ in 3:16) {
  segments(
    x0 = educ,
    y0 = conf.intervals[1, educ-2],
    x1 = educ, 
    y1 = conf.intervals[2, educ-2],
    lwd = 20)
}



###### QUESTION 4 ##########################################################

#STEP 1: Fit the regression model. Report and interpret the regression coefficient and 95% confidence intervals for age and education
glm1 <- glm(treat~age+educ+hisp+re74+re75,data=lalonde,family=binomial)
summary(glm1)
confint(glm1)

#STEP 2: Bootstrap
library(boot)
boot_coef <- function(data,index){
  coef(glm(treat~age+educ+hisp+re74+re75,data=lalonde,subset=index,family=binomial))
}

b <- boot(lalonde,boot_coef,1000)

###calculate confidence interval for the first coefficient:age
boot.ci(boot.out = b,index = 1,type = "perc")

###calculate confidence interval for the second coefficient:educ
boot.ci(boot.out = b,index = 2,type = "perc")

#STEP 3: 95% confident interval plot for expected probability

data("lalonde")
glm1 <- glm(treat~age+educ+hisp+re74+re75,data=lalonde,family=binomial)
sim.me <- sim(glm1,1000)

get_predicted_prob <- function(coefs,people){
  logit <- coefs[1]+people[1]*coefs[2]+people[2]*coefs[3]+people[3]*coefs[4]+people[4]*coefs[5]+people[5]*coefs[6]
  return(exp(logit)/(1+exp(logit)))
}

###build a storage matrix to store the 1000 (expected probabilities) * 14 (education years) values
store2.matrix <- matrix(NA,nrow = 1000,ncol = 14)

###repeat 1000 times for each predicted probability to get average, aka expected probability
for(m in 1:1000){
###use the 1000 simulated coefficients to get 1000 predicted probabilities, for each education year
  for (educ in c(3:16)) {
     for (i in 1:1000){
      people_educ <- c(mean(lalonde$age),educ,mean(lalonde$hisp),mean(lalonde$re74),mean(lalonde$re75))
      store1.matrix[i,educ-2] <- get_predicted_prob(sim.me@coef[i, ],people_educ)
    store2.matrix[m,educ-2] <- mean(get_predicted_prob(sim.me@coef[m, ],people_educ))
    }
  }
}

###get the confidence intervals of 1000 expected probabilities, for each education year
conf.intervals_exp_prob <- apply(store2.matrix, 2, quantile, probs = c(0.025, 0.975))
conf.intervals_exp_prob

###plot the data, use a segment function
plot(x = c(1:1000), y = c(1:1000), type = "n", xlim = c(3,16), ylim = c(0,1), 
     main = "95% Confidence Intervals of Expected Probability of Treatment as Years of Education Varies", xlab = "Years of Education", 
     ylab = "Expected Probability of Treatment")


for (educ in 3:16) {
  segments(
    x0 = educ,
    y0 = conf.intervals_exp_prob[1, educ - 2],
    x1 = educ,
    y1 = conf.intervals_exp_prob[2, educ - 2],
    lwd = 20)
}


#STEP 4: 95% confident interval plot for predicted probability

###simulate the regression model to get simulated coefficients 1000 times
sim.me <- sim(glm1,1000)

###build a storage matrix to store the 1000 (predicted probabilities) * 14 (education years) values
store1.matrix <- matrix(NA,nrow = 1000,ncol = 14)

###build a function to get predicted probabilities for a particular type of people profile
get_predicted_prob <- function(coefs,people){
  logit <- coefs[1]+people[1]*coefs[2]+people[2]*coefs[3]+people[3]*coefs[4]+people[4]*coefs[5]+people[5]*coefs[6]
  return(exp(logit)/(1+exp(logit)))
}

###use the 1000 simulated coefficients to get 1000 predicted probabilities, for each education year
for (educ in c(3:16)) {
  for (i in 1:1000){
  people_educ <- c(mean(lalonde$age),educ,mean(lalonde$hisp),mean(lalonde$re74),mean(lalonde$re75))
  store1.matrix[i,educ-2] <- get_predicted_prob(sim.me@coef[i, ],people_educ)
  }
}

###get the confidence intervals of 1000 predictions, for each education year
conf.intervals_predict_prob <- apply(store1.matrix, 2, quantile, probs = c(0.025, 0.975))
conf.intervals_predict_prob

###plot the data, use a segment function
plot(x = c(1:1000), y = c(1:1000), type = "n", xlim = c(3,16), ylim = c(0,1), 
     main = "95% Confidence Intervals of Predicted Probability of Treatment as Years of Education Varies", xlab = "Years of Education", 
     ylab = "Predicted Probability of Treatment")


for (educ in 3:16) {
  segments(
    x0 = educ,
    y0 = conf.intervals_predict_prob[1, educ - 2],
    x1 = educ,
    y1 = conf.intervals_predict_prob[2, educ - 2],
    lwd = 20)
}


###### QUESTION 5 ##########################################################
##no code

###### QUESTION 6 ##########################################################

#STEP 1: Import & Clean the Data

ksdata <- na.omit(read.csv("https://tinyurl.com/KaggleDataCS112",header = T,na.strings=c(""," ","NA")))

#STEP 2: Codify outcome variable

##create a binary dummy variable success, and assign all other inputs except for"successul" into the "0" category
success <- ifelse(ksdata$state=="successful",1,0)

#STEP 3: Getting the project length variable

library(lubridate)

##calculate the days between launch date and deadline date
length <- time_length(interval(ksdata$launched,ksdata$deadline),"days")

##create a data frame with new variables
ksdata1 <- data.frame(ksdata,success,length)

##select and create a subset of the data
ksdata2 <- subset(ksdata1,length <60,select=ID:length)

#STEP 4: Splitting the data into a training and a testing set

##80% of the sample size
sample_size <- floor(0.8 * nrow(ksdata2))

##randomly assign 80% data into training data, and the rest to test data
set.seed(1)
train_ind <- sample(seq_len(nrow(ksdata2)), size = sample_size)
train <- ksdata2[train_ind, ]
test <- ksdata2[-train_ind, ]

#STEP 5: Fitting a model

##fix all the formats of independent variables
ksdata2$backers <- as.numeric(ksdata2$backers)
ksdata2$main_category <- as.factor(ksdata2$main_category)
ksdata2$usd_goal_real<-as.numeric(ksdata2$usd_goal_real)

##fit a logistic regression model
ks.glm <- glm(success~ backers+main_category+usd_goal_real+length,data=train,family = binomial)
summary(ks.glm)

#STEP 6: Predictions

##use the test data and the regression model to predict outcome variable probabilities, and convert to a binary result 
glm.probs.test <- predict(ks.glm,newdata = test,type = "response")
glm.pred.test <- rep("0",39507)

##under the rule that any probability above 50% will be assigned to "1", if else to "0"
glm.pred.test[glm.probs.test>0.5]="1"

##use the training data and the regression model to predict outcome variable probabilities, and convert to a binary result
glm.probs.train <- predict(ks.glm,type = "response")
glm.pred.train <- rep("0",158024)

##under the rule that any probability above 50% will be assigned to "1", if else to "0"
glm.pred.train[glm.probs.train>0.5]="1"

#STEP 7: How well did it do?

##create a confusion table to compare testing results, calculate misclassification rate for training and test data

##test set
table(glm.pred.test,test$success)
misclassification_rate_test <- mean(glm.pred.test!=test$success)
misclassification_rate_test

##training set
table(glm.pred.train,train$success)
misclassification_rate_train <- mean(glm.pred.train!=train$success)
misclassification_rate_train

#STEP 7:LOOCV method

##take 5% of the total data set as the data set for LOOCV
library(boot)
sample_size_loocv <- floor(0.05 * nrow(ksdata2))
set.seed(1)
train_ind_loocv <- sample(seq_len(nrow(ksdata2)), size = sample_size_loocv)
train_loocv <- ksdata2[train_ind_loocv, ]

##use the loocv training data (5% of total) to fit a new logistic regression model
ks.glm.loocv <- glm(success~ backers+main_category+usd_goal_real+length,data = train_loocv,family = binomial)

##use the cost function that is for binary results, with the 50% rule
cost <- function(r, pi) mean(abs(r-pi)> 0.5)

##get the cross-validation error rate
cv.err=cv.glm(train_loocv,ks.glm.loocv, cost = cost)
cv.err$delta[1]




